# -*- coding: utf-8 -*-
"""LogRegNumpy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115lUO4EVWM2PtSfGWFW8c45bhja9rPEE

# Logistic unit in NumPy

> Блок с отступами

* In this task you are supposed to implement forward and backward passes for logistic unit in `forward_backward_pass` function and also the `predict` function.
* You can use all other avaliable `LogisticRegression`'s methods and you don't have to modify them.
* Using additional modules is prohibited.
"""

# this is that you can use
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

accuracy_score([1, 1, 0, 0, 1],[0, 1, 0, 0, 0]) #пример вычета точности по векторам.

np.random.randn(1, 5)

x = np.array([1, 2, 3]).reshape(3, 1)

class LogisticRegression:
    def __init__(self):
        # variables for storing weights
        self.W, self.b = None, None
        # variable for storing current loss 
        self.loss = None
        
    def cost_function(self, p, y):
        # cost function (log loss) 
        # Кросс-энтропия(функция ошибки)
        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p) )
        
    def _init_weights(self):
        # initialize normal
        self.W = np.random.randn(3, 1) 
        self.b = np.random.randn()
        
    def sigm(self, x):
        # sigmoid (logistic) function
        return 1 / (1 + np.exp(-x))
            
    def forward_backward_pass(self, x, y, eta):
        """
        This function implements forward and backward pass and updates parameters W and b

        """
        # FORWARD
        linear_pred = np.dot(x, self.W) + self.b
        y_pred = self.sigm(linear_pred)
        # FORWARD ENDS
        
        # calculate loss 
        self.loss = self.cost_function(y_pred, y)
        
        
        # BACKWARD
        # here you need to calculate all the derivatives of loss with respect to W and b
        
        dLdW = (y_pred - y) * x.T 
        dLdb = (y_pred - y)
        
        # then update W and b
        # don't forget the learning rate - 'eta'!
        # Из наших переменных(вес, байс)
        self.W = self.W - eta * dLdW
        self.b = self.b - eta * dLdb
        
        # BACKWARD ENDS
        

        
    def fit(self, X, Y, eta=0.01, decay=0.999, iters=1000):
        self._init_weights()
        
        # buffer - for printing out mean loss over 100 iterations
        buffer = []
        # L - history of losses
        L = []
        # A - acc history 
        A = []
        
        # perform iterative gradient descent
        for i in range(iters):
            index = np.random.randint(0, len(X))
            x = X[index]
            y = Y[index]
            # update params
            self.forward_backward_pass(x, y, eta)
            # update learning rate
            eta *= decay
            
            L.append(self.loss)
            buffer.append(self.loss)
            #acc = accuracy_score(y, self.predict(X))

            #A.append(acc)
            if i % 100 == 0:
                print('Mean log loss:', np.mean(buffer))
                buffer = []
        
        return A, L
    
    def predict(self, x):
        # Note you have to return actual classes (not probs)
        linear_pred = np.dot(x, self.W) + self.b
        y_pred = self.sigm(linear_pred)
        return np.round(y_pred)

"""# Let's check how your model performs"""

# generate some synthetic data with 10 features
X = np.random.randn(100, 3) 
y = np.array(list(map(lambda x: np.sum(x) > 0, X))).reshape(-1, 1).astype(np.int32)

log_reg = LogisticRegression()
# feel free to play with fit params
acc, losses = log_reg.fit(X, y, iters=1000)

plt.figure(figsize=(10, 8))
''
''
plt.plot(losses, label='loss')
plt.xlim(0, len(losses))
plt.grid()
plt.legend()
plt.show()

